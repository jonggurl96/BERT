{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "- HuggingFace에서 제공하는 대부분의 transformer 모델들은 텍스트를 바로 입력받을 수 없다. BERT 모델의 경우 Tokenizer로 만든 Token들을 고유한 정수로 바꾼 후 입력으로 받는다.\n",
    "- HuggingFace는 각 모델별로 tokenizer를 제공하며 이는 자신의 짝인 모델이 어떤 형태의 입력을 요구하는지 알고있어 tokenizer의 출력을 모델에 넣어주기만 하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T03:26:43.979110Z",
     "start_time": "2023-07-27T03:26:35.805353100Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 해당 모델의 tokenizer 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize할 문자열을 tokenizer에 전달인자로 주기만 하면 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T01:29:20.826648100Z",
     "start_time": "2023-07-27T01:29:20.822868800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids: [101, 1045, 2293, 17953, 2361, 999, 102]\n",
      "token type ids: [0, 0, 0, 0, 0, 0, 0]\n",
      "attention mask: [1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "r\"\"\"\n",
    "    BatchEncoding 객체이지만 dictionary라 생각해도 무방함.\n",
    "    추가적인 3 전달인자의 default 값은 True\n",
    "\"\"\"\n",
    "tokens = tokenizer(\"I love NLP!\",\n",
    "\t\t\t\t   return_token_type_ids = True,\n",
    "\t\t\t\t   return_attention_mask = True,\n",
    "\t\t\t\t   add_special_tokens = True)\n",
    "\n",
    "print(f\"input ids: {tokens['input_ids']}\")\n",
    "print(f\"token type ids: {tokens['token_type_ids']}\")\n",
    "print(f\"attention mask: {tokens['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token id를 token으로 변환할 수 있다.\n",
    "\n",
    "nlp 단어가 nl과 p로 나뉜 까닭은 BERT tokenizer가 [WordPiece 알고리즘](https://huggingface.co/docs/transformers/tokenizer_summary#wordpiece)에 따라 각 subword 단위로 토큰을 분리하기 때문이다.\n",
    "\n",
    "반대로 토큰을 id로 바꿀 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T01:29:23.590943200Z",
     "start_time": "2023-07-27T01:29:23.589434400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "['[CLS]', 'i', 'love', 'nl', '##p', '!', '[SEP]']\n",
      "101\n",
      "[101, 1045, 2293, 17953, 2361, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "converted_token = tokenizer.convert_ids_to_tokens(tokens.input_ids[0])\n",
    "converted_tokens = tokenizer.convert_ids_to_tokens(tokens.input_ids)\n",
    "\n",
    "print(converted_token)\n",
    "print(converted_tokens)\n",
    "\n",
    "print(tokenizer.convert_tokens_to_ids(converted_token))\n",
    "print(tokenizer.convert_tokens_to_ids(converted_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T01:29:25.449786200Z",
     "start_time": "2023-07-27T01:29:24.584832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded >>> [CLS] i love nlp! [SEP]\n",
      "Decoded skipped special tokens >>> i love nlp!\n"
     ]
    }
   ],
   "source": [
    "# special token을 포함해 문자열로 변환하기\n",
    "encoded_tokens = tokenizer(\"I love NLP!\")['input_ids']\n",
    "print(f\"Decoded >>> {tokenizer.decode(encoded_tokens)}\")\n",
    "print(f\"Decoded skipped special tokens >>> {tokenizer.decode(encoded_tokens, skip_special_tokens = True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bert Special Tokens]\n",
    "- padding: [PAD] (0) - tokenizeer.pad_token\n",
    "- unknown: [UNK] (100) - tokenizer.unk_token\n",
    "- classifier: [CLS] (101) - tokenizer.cls_token\n",
    "- seperator: [SEP] (102) - tokenizer.sep_token\n",
    "- mask: [MASK] (103) - tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T01:29:25.646130600Z",
     "start_time": "2023-07-27T01:29:25.643623900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 102, 0, 101, 103]\n",
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.all_special_ids)\n",
    "print(tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer의 입력에 List[str] 형태의 값이 들어오면 batch 입력이라 이해한다.\n",
    "\n",
    "이 때 BERT는 문장A와 문장B, 2개의 문장만 입력받을 수 있다.\n",
    "\n",
    "`List[List[str]]`의 꼴일 경우 [(문장A, 문장B), (...), ...]의 꼴로 인식하여 token_type_ids가 0과 1인 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T01:29:27.117136900Z",
     "start_time": "2023-07-27T01:29:27.109378500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [[101, 1045, 2903, 102, 1045, 2064, 4875, 102], [101, 1045, 2031, 1037, 7279, 102, 1045, 2031, 2019, 6207, 102]], 'token_type_ids': [[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\n",
    "\t[\"I believe\", \"I can fly\"],\n",
    "\t[\"I have a pen\", \"I have an apple\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "return_tensors 옵션을 주면 torch_tensor 형태 출력을 받을 수도 있다.\n",
    "\n",
    "batch 입력 시 각 텍스트의 token 수가 다르다면 pytorch tensor로 출력을 받을 수 없다.\n",
    "\n",
    "이를 해결하기 위해 최대 길이를 정하고 패딩 또는 자르기를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T01:29:29.155783700Z",
     "start_time": "2023-07-27T01:29:29.152679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[  101,  7279,  7222, 23804,  6207,  7279,   102,     0],\n        [  101,  2008,  2015,  2025,  2129,  1045,  2293,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1]])}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Pen Pineapple Apple Pen\", \"Thats not how i love you\"],\n",
    "\t\t  return_tensors = \"pt\",\n",
    "\t\t  max_length = 8,\n",
    "\t\t  padding = \"max_length\",\n",
    "\t\t  truncation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BertModel\n",
    "\n",
    "Bert는 다양한 task에 대해 fine-tuning하여 사용할 수 있으며 방법은 일반적으로 다음과 같다.\n",
    "\n",
    "1. BERT 위에 해당 task를 위한 추가 layer(head)를 쌓는다.\n",
    "2. 이렇게 만들어진 모델을 추가 데이터로 학습시킨다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BertModel\n",
    "\n",
    "HuggingFace에서 head가 없는 순수 BERT\n",
    "\n",
    "1. add_pooling_layer: BERT 위에 추가적으로 pooling layer를 쌓을 것인지 여부(default: True), [CLS] 토큰의 embedding만을 뽑아 linear 연산과 activation 연산을 수행하는 layer로, BERT를 이용해 classification task를 수행할 때 주로 사용한다.\n",
    "2. output_hidden_states: 각 layer의 hidden states의 출력 여부(default: False)\n",
    "3. output_attentions: 각 layer의 attention distribution(attention weight)의 배열 attentions의 출력 여부(default: False)\n",
    "\n",
    "~~`Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: [...]`: bert-base-uncased의 checkpoint의 가중치와 BertModel의 가중치가 호환되지 않아 나열된 가중치들을 버렸다는 것을 알려주는 경고, cls header의 가중치이므로 무시~~"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\",\n",
    "\t\t\t\t\t\t\t\t  add_pooling_layer = False,\n",
    "\t\t\t\t\t\t\t\t  output_hidden_states = True,\n",
    "\t\t\t\t\t\t\t\t  output_attentions = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-27T03:27:57.881664600Z",
     "start_time": "2023-07-27T03:27:42.603244Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.num_parameters(): HuggingFace의 BERT 모델은 약 108M개의 파라미터로 이루어져 있다.\n",
      "108891648\n",
      "model의 구조는 다음과 같다.\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"model.num_parameters(): HuggingFace의 BERT 모델은 약 108M개의 파라미터로 이루어져 있다.\\n{model.num_parameters()}\")\n",
    "print(f\"model의 구조는 다음과 같다.\\n{model}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-27T03:28:09.913340900Z",
     "start_time": "2023-07-27T03:28:09.911833400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BertModel을 사용하는 방법\n",
    "\n",
    "일반적인 torch module(torch.nn.Module)을 쓰듯이 tokenizer로 만든 입력값을 넣어주면 된다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_input: {'input_ids': tensor([[  101,  1045,  2293, 17953,  2361,   999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "BertModel의 출력값에 포함된 키들: odict_keys(['last_hidden_state', 'hidden_states', 'attentions'])\n"
     ]
    }
   ],
   "source": [
    "model_input = tokenizer(\"I love NLP!\", return_tensors = \"pt\")\n",
    "output = model(**model_input)\n",
    "\n",
    "print(f\"model_input: {model_input}\")\n",
    "\n",
    "print(f\"BertModel의 출력값에 포함된 키들: {output.keys()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-27T03:28:16.604869200Z",
     "start_time": "2023-07-27T03:28:14.856098400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- hidden_states: 각 layer의 hidden_state를 모아놓은 리스트.\n",
    "- attentions: 각 layer의 attention weight를 모아놓은 리스트."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_states 갯수: 13\n",
      "tensor([[[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]]])\n",
      "hidden_state.shape: torch.Size([1, 7, 768])\n",
      "attention 갯수: 12\n",
      "attention.shape: torch.Size([1, 12, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "print(f\"hidden_states 갯수: {len(output.hidden_states)}\")\n",
    "print(output.hidden_states[-1] == output.last_hidden_state)\n",
    "print(f\"hidden_state.shape: {output.hidden_states[0].shape}\")\n",
    "\n",
    "print(f\"attention 갯수: {len(output.attentions)}\")\n",
    "print(f\"attention.shape: {output.attentions[0].shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-27T03:44:18.479753300Z",
     "start_time": "2023-07-27T03:44:18.319996600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
